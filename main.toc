\babel@toc {french}{}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}About this course}{5}{section.1.1}%
\contentsline {section}{\numberline {1.2}Cours Grading}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}How to be efficient and do well in this course}{5}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Initial case: Finite $\Omega $: set of all possiblie outcomes}{6}{section.1.3}%
\contentsline {section}{\numberline {1.4}Conditional Probability}{6}{section.1.4}%
\contentsline {paragraph}{Conditional probability}{6}{section*.3}%
\contentsline {paragraph}{Independent Events}{6}{section*.4}%
\contentsline {paragraph}{General Case: Finite $\Omega $, arbitary $p(\omega )$}{6}{section*.5}%
\contentsline {section}{\numberline {1.5}Conditional probability and Independent Events}{7}{section.1.5}%
\contentsline {paragraph}{General form}{7}{section*.6}%
\contentsline {paragraph}{Independet events}{7}{section*.7}%
\contentsline {paragraph}{Disjoin event}{7}{section*.8}%
\contentsline {paragraph}{Law of total probability}{7}{section*.9}%
\contentsline {paragraph}{Bays' Rule}{7}{section*.10}%
\contentsline {section}{\numberline {1.6}Random variable}{8}{section.1.6}%
\contentsline {paragraph}{Random variable}{8}{section*.11}%
\contentsline {paragraph}{Probability distribution}{8}{section*.12}%
\contentsline {subsection}{\numberline {1.6.1}Two random variables}{8}{subsection.1.6.1}%
\contentsline {paragraph}{Two random variables}{8}{section*.13}%
\contentsline {section}{\numberline {1.7}Expected Value}{9}{section.1.7}%
\contentsline {paragraph}{Expected value}{9}{section*.14}%
\contentsline {paragraph}{linearity}{9}{section*.15}%
\contentsline {paragraph}{Random variable and independecy}{9}{section*.16}%
\contentsline {paragraph}{Generalization}{9}{section*.17}%
\contentsline {paragraph}{Condition probability}{9}{section*.18}%
\contentsline {paragraph}{Independent random variables}{9}{section*.19}%
\contentsline {paragraph}{Expected value and operation}{10}{section*.21}%
\contentsline {section}{\numberline {1.8}Entropy}{10}{section.1.8}%
\contentsline {paragraph}{Introduction}{10}{section*.22}%
\contentsline {paragraph}{Definition}{10}{section*.23}%
\contentsline {paragraph}{Few comments}{10}{section*.24}%
\contentsline {paragraph}{Example}{11}{section*.25}%
\contentsline {subsection}{\numberline {1.8.1}Information-Theory Inequality}{11}{subsection.1.8.1}%
\contentsline {paragraph}{Lemma (IT-Inequality)}{11}{section*.26}%
\contentsline {paragraph}{Entropy Bounds}{11}{section*.27}%
\contentsline {subsection}{\numberline {1.8.2}Random variables and Entropy}{11}{subsection.1.8.2}%
\contentsline {paragraph}{$n$ random variable}{11}{section*.28}%
\contentsline {paragraph}{1.4 of textbooks}{11}{section*.29}%
\contentsline {paragraph}{Ex hat party 1950}{12}{section*.31}%
\contentsline {paragraph}{Entropy}{12}{section*.32}%
\contentsline {subsection}{\numberline {1.8.3}Entropy bounds}{12}{subsection.1.8.3}%
\contentsline {paragraph}{Bound}{12}{section*.33}%
\contentsline {section}{\numberline {1.9}Source Coding Purpose}{12}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}Setup}{12}{subsection.1.9.1}%
\contentsline {paragraph}{Setup}{12}{section*.34}%
\contentsline {paragraph}{Example}{12}{section*.35}%
\contentsline {paragraph}{Decodability}{13}{section*.36}%
\contentsline {paragraph}{Prefix Free codes}{13}{section*.37}%
\contentsline {paragraph}{Code for one random variable}{13}{section*.38}%
\contentsline {paragraph}{Complete tree of a code}{13}{section*.39}%
\contentsline {paragraph}{Binary tree}{13}{section*.40}%
\contentsline {paragraph}{Ternary Tree}{13}{section*.41}%
\contentsline {paragraph}{With/Without prefix}{13}{section*.42}%
\contentsline {paragraph}{Decoding tree}{13}{section*.43}%
\contentsline {subsection}{\numberline {1.9.2}Codeword length}{14}{subsection.1.9.2}%
\contentsline {subsection}{\numberline {1.9.3}Kraft McMillan}{14}{subsection.1.9.3}%
\contentsline {paragraph}{Part 1. Necessary condition for the code to be uniquely decodable}{14}{section*.44}%
\contentsline {paragraph}{Recall Kraft McMillan}{14}{section*.45}%
\contentsline {paragraph}{Proof of K-MM Part I}{14}{section*.46}%
\contentsline {subsection}{\numberline {1.9.4}Important Consequence of Kraft McMillan}{15}{subsection.1.9.4}%
\contentsline {paragraph}{Part I}{15}{section*.47}%
\contentsline {paragraph}{Part II}{15}{section*.48}%
\contentsline {paragraph}{Prefix free codes}{15}{section*.49}%
\contentsline {paragraph}{Average Codeword length}{15}{section*.50}%
\contentsline {paragraph}{Average codeword length: Lower Bound}{16}{section*.51}%
\contentsline {paragraph}{Key observation}{17}{section*.53}%
\contentsline {paragraph}{Theorem}{17}{section*.54}%
\contentsline {paragraph}{Key Idea}{17}{section*.56}%
\contentsline {paragraph}{Our Next Nugget}{18}{section*.57}%
\contentsline {paragraph}{KEy(simple) Independent}{18}{section*.58}%
\contentsline {paragraph}{Not independent}{18}{section*.59}%
\contentsline {paragraph}{Conditional Probability}{18}{section*.60}%
\contentsline {paragraph}{Conditional Expectation of $X$ given $Y = y$}{18}{section*.61}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y = y$}{18}{section*.62}%
\contentsline {paragraph}{Entropy Bounds}{19}{section*.63}%
\contentsline {paragraph}{Example}{19}{section*.64}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y$}{19}{section*.65}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y$}{20}{section*.66}%
\contentsline {paragraph}{Conditioning Reduces Entropy}{20}{section*.67}%
\contentsline {paragraph}{Conditional Entropy of $f(x)$}{21}{section*.68}%
\contentsline {paragraph}{Conditioning reduced Entropy}{21}{section*.69}%
