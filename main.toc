\babel@toc {english}{}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}About this course}{5}{section.1.1}%
\contentsline {section}{\numberline {1.2}Cours Grading}{5}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}How to be efficient and do well in this course}{5}{subsection.1.2.1}%
\contentsline {chapter}{\numberline {2}Entropy}{7}{chapter.2}%
\contentsline {section}{\numberline {2.1}Initial case: Finite $\Omega $: set of all possiblie outcomes}{7}{section.2.1}%
\contentsline {section}{\numberline {2.2}Conditional Probability}{7}{section.2.2}%
\contentsline {paragraph}{Conditional probability}{7}{section*.3}%
\contentsline {paragraph}{Independent Events}{7}{section*.4}%
\contentsline {paragraph}{General Case: Finite $\Omega $, arbitary $p(\omega )$}{7}{section*.5}%
\contentsline {section}{\numberline {2.3}Conditional probability and Independent Events}{8}{section.2.3}%
\contentsline {paragraph}{General form}{8}{section*.6}%
\contentsline {paragraph}{Independet events}{8}{section*.7}%
\contentsline {paragraph}{Disjoin event}{8}{section*.8}%
\contentsline {paragraph}{Law of total probability}{8}{section*.9}%
\contentsline {paragraph}{Bays' Rule}{9}{section*.10}%
\contentsline {section}{\numberline {2.4}Random variable}{9}{section.2.4}%
\contentsline {paragraph}{Random variable}{9}{section*.11}%
\contentsline {paragraph}{Probability distribution}{9}{section*.12}%
\contentsline {subsection}{\numberline {2.4.1}Two random variables}{10}{subsection.2.4.1}%
\contentsline {paragraph}{Two random variables}{10}{section*.13}%
\contentsline {section}{\numberline {2.5}Expected Value}{10}{section.2.5}%
\contentsline {paragraph}{Expected value}{10}{section*.14}%
\contentsline {paragraph}{linearity}{10}{section*.15}%
\contentsline {paragraph}{Random variable and independecy}{10}{section*.16}%
\contentsline {paragraph}{Generalization}{10}{section*.17}%
\contentsline {paragraph}{Condition probability}{10}{section*.18}%
\contentsline {paragraph}{Independent random variables}{10}{section*.19}%
\contentsline {paragraph}{Expected value and operation}{11}{section*.21}%
\contentsline {section}{\numberline {2.6}Entropy}{11}{section.2.6}%
\contentsline {paragraph}{Introduction}{11}{section*.22}%
\contentsline {paragraph}{Definition}{11}{section*.23}%
\contentsline {paragraph}{Few comments}{12}{section*.24}%
\contentsline {paragraph}{Example}{12}{section*.25}%
\contentsline {subsection}{\numberline {2.6.1}Information-Theory Inequality}{12}{subsection.2.6.1}%
\contentsline {paragraph}{Lemma (IT-Inequality)}{12}{section*.26}%
\contentsline {paragraph}{Entropy Bounds}{12}{section*.27}%
\contentsline {subsection}{\numberline {2.6.2}Random variables and Entropy}{13}{subsection.2.6.2}%
\contentsline {paragraph}{$n$ random variable}{13}{section*.28}%
\contentsline {paragraph}{1.4 of textbooks}{13}{section*.29}%
\contentsline {paragraph}{Ex hat party 1950}{13}{section*.31}%
\contentsline {paragraph}{Entropy}{13}{section*.32}%
\contentsline {subsection}{\numberline {2.6.3}Entropy bounds}{14}{subsection.2.6.3}%
\contentsline {paragraph}{Bound}{14}{section*.33}%
\contentsline {section}{\numberline {2.7}Source Coding Purpose}{14}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Setup}{14}{subsection.2.7.1}%
\contentsline {paragraph}{Setup}{14}{section*.34}%
\contentsline {paragraph}{Example}{14}{section*.35}%
\contentsline {paragraph}{Decodability}{14}{section*.36}%
\contentsline {paragraph}{Prefix Free codes}{14}{section*.37}%
\contentsline {paragraph}{Code for one random variable}{14}{section*.38}%
\contentsline {paragraph}{Complete tree of a code}{14}{section*.39}%
\contentsline {paragraph}{Binary tree}{14}{section*.40}%
\contentsline {paragraph}{Ternary Tree}{15}{section*.41}%
\contentsline {paragraph}{With/Without prefix}{15}{section*.42}%
\contentsline {paragraph}{Decoding tree}{15}{section*.43}%
\contentsline {subsection}{\numberline {2.7.2}Codeword length}{15}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Kraft McMillan}{15}{subsection.2.7.3}%
\contentsline {paragraph}{Part 1. Necessary condition for the code to be uniquely decodable}{15}{section*.44}%
\contentsline {paragraph}{Recall Kraft McMillan}{15}{section*.45}%
\contentsline {paragraph}{Proof of K-MM Part I}{16}{section*.46}%
\contentsline {subsection}{\numberline {2.7.4}Important Consequence of Kraft McMillan}{16}{subsection.2.7.4}%
\contentsline {paragraph}{Part I}{16}{section*.47}%
\contentsline {paragraph}{Part II}{16}{section*.48}%
\contentsline {paragraph}{Prefix free codes}{16}{section*.49}%
\contentsline {paragraph}{Average Codeword length}{17}{section*.50}%
\contentsline {paragraph}{Average codeword length: Lower Bound}{17}{section*.51}%
\contentsline {paragraph}{Key observation}{18}{section*.53}%
\contentsline {paragraph}{Theorem}{18}{section*.54}%
\contentsline {paragraph}{Key Idea}{19}{section*.56}%
\contentsline {paragraph}{Our Next Nugget}{19}{section*.57}%
\contentsline {paragraph}{KEy(simple) Independent}{19}{section*.58}%
\contentsline {paragraph}{Not independent}{19}{section*.59}%
\contentsline {paragraph}{Conditional Probability}{19}{section*.60}%
\contentsline {paragraph}{Conditional Expectation of $X$ given $Y = y$}{19}{section*.61}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y = y$}{20}{section*.62}%
\contentsline {paragraph}{Entropy Bounds}{20}{section*.63}%
\contentsline {paragraph}{Example}{20}{section*.64}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y$}{21}{section*.65}%
\contentsline {paragraph}{Conditional Entropy of $X$ given $Y$}{21}{section*.66}%
\contentsline {paragraph}{Conditioning Reduces Entropy}{21}{section*.67}%
\contentsline {paragraph}{Conditional Entropy of $f(x)$}{22}{section*.68}%
\contentsline {paragraph}{Conditioning reduced Entropy}{22}{section*.69}%
\contentsline {paragraph}{Main definitions}{23}{section*.71}%
\contentsline {paragraph}{Lisa rolls two dice}{24}{section*.72}%
\contentsline {paragraph}{The chain rule for entropy}{25}{section*.73}%
\contentsline {paragraph}{The chain rule entropy}{25}{section*.74}%
\contentsline {paragraph}{Another way around}{26}{section*.75}%
\contentsline {paragraph}{Example}{27}{section*.76}%
\contentsline {subsection}{\numberline {2.7.5}Random Processes}{27}{subsection.2.7.5}%
\contentsline {paragraph}{A.K.A Source models}{27}{section*.77}%
\contentsline {paragraph}{Experience little play}{28}{section*.79}%
\contentsline {paragraph}{Last Week}{28}{section*.80}%
\contentsline {paragraph}{The 20 question problem}{29}{section*.81}%
\contentsline {paragraph}{Solution}{29}{section*.82}%
\contentsline {paragraph}{Optimality}{29}{section*.83}%
\contentsline {paragraph}{Sorting via pairwise comparisons}{30}{section*.84}%
\contentsline {paragraph}{Billard Balls}{31}{section*.85}%
\contentsline {paragraph}{Fact 1}{32}{section*.86}%
\contentsline {paragraph}{Fact 2}{33}{section*.87}%
\contentsline {paragraph}{Example}{33}{section*.88}%
\contentsline {paragraph}{Billard Balls}{34}{section*.90}%
\contentsline {paragraph}{Strategies}{34}{section*.91}%
\contentsline {subsection}{\numberline {2.7.6}Prediction, Learning and cross-Entropy Loss}{34}{subsection.2.7.6}%
\contentsline {paragraph}{Example}{34}{section*.92}%
\contentsline {paragraph}{Cross entropy loss}{35}{section*.93}%
\contentsline {paragraph}{A (very) simple neural network}{36}{section*.94}%
\contentsline {paragraph}{For a single image $ \mathcal {X}$}{36}{section*.95}%
\contentsline {paragraph}{Cross entropy loss}{36}{section*.96}%
\contentsline {section}{\numberline {2.8}Summary of chapter 1}{37}{section.2.8}%
\contentsline {paragraph}{Entropy}{37}{section*.97}%
\contentsline {paragraph}{Data Compression}{37}{section*.98}%
\contentsline {paragraph}{Models}{37}{section*.99}%
\contentsline {paragraph}{Entropy and algorithm}{38}{section*.100}%
\contentsline {paragraph}{Cross-Entropy Loss}{38}{section*.101}%
\contentsline {chapter}{\numberline {3}Cryptography}{39}{chapter.3}%
\contentsline {section}{\numberline {3.1}One-Time pad, Perfect Secrecy, Public-Key}{39}{section.3.1}%
\contentsline {paragraph}{Why cryptography}{39}{section*.103}%
\contentsline {paragraph}{Basic setup for condidentiality}{39}{section*.104}%
\contentsline {paragraph}{Basic Terminology}{39}{section*.105}%
\contentsline {paragraph}{Ancient cryptography}{39}{section*.106}%
\contentsline {paragraph}{The one-time pad}{40}{section*.107}%
\contentsline {paragraph}{Perfect secrecy}{41}{section*.108}%
\contentsline {paragraph}{Perfect secrecy of the one time pad}{41}{section*.109}%
\contentsline {paragraph}{Weakness of the one time pad}{41}{section*.110}%
\contentsline {paragraph}{Perfect secrecy requires high entropy keys}{42}{section*.111}%
